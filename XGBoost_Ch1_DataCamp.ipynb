{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 1: XGBoost (DataCamp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Course notes from [DataCamp](https://campus.datacamp.com/courses/extreme-gradient-boosting-with-xgboost) XGBoost<br>\n",
    "Importing xgboost requires some [work](https://www.ibm.com/developerworks/community/blogs/jfp/entry/Installing_XGBoost_For_Anaconda_on_Windows?lang=en)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost: Binary Logistic Classifier (Customer Churn)\n",
    "Introduction to XGBoost working with churn data. This dataset contains imaginary data from a ride-sharing app with user behaviors over their first month of app usage in a set of imaginary cities as well as whether they used the service 5 months after sign-up. The goal is to use the first month's worth of data to __predict whether the app's users will remain users__ of the service at the 5 month mark. This is a typical setup for a churn prediction problem. To do this, split the data into training and test sets, fit a small xgboost model on the training set, and evaluate its performance on the test set by computing its accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# may be required if xgboost import throws errors \n",
    "# import os\n",
    "# mingw_path = 'C:\\\\Program Files\\\\mingw-w64\\\\x86_64-7.2.0-posix-seh-rt_v5-rev1\\\\mingw64\\\\bin'\n",
    "# os.environ['PATH'] = mingw_path + ';' + os.environ['PATH']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load data (may have to re-point once in github)\n",
    "churn_data = pd.read_csv(\"https://s3.amazonaws.com/assets.datacamp.com/production/course_3739/datasets/taxi_churn_data_dummified.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create df for the features and the target: X, y\n",
    "X, y = churn_data.iloc[:,:-1], churn_data.iloc[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy (unpreprocessed) with XGBoost: 0.743300\n"
     ]
    }
   ],
   "source": [
    "# Create the training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 123)\n",
    "\n",
    "# Instantiate the XGBClassifier: xg_cl\n",
    "xg_cl = xgb.XGBClassifier(objective = 'binary:logistic', n_estimators = 10, seed = 123)\n",
    "\n",
    "# fit model\n",
    "xg_cl.fit(X_train, y_train)\n",
    "\n",
    "# predictions based on X_test\n",
    "preds = xg_cl.predict(X_test)\n",
    "\n",
    "# manual calc for accuracy\n",
    "accuracy = float(np.sum(preds == y_test))/y_test.shape[0]\n",
    "\n",
    "print ('Accuracy (unpreprocessed) with XGBoost: %f' % (accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-XGBoost Decision Tree and LogReg (Breast Cancer Wisconsin Dataset)\n",
    "Simple decision tree using scikit-learn's DecisionTreeClassifier on UCI ML Breast Cancer Wisconsin (Diagnostic) dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# may be required if xgboost import throws errors \n",
    "# import os\n",
    "# mingw_path = 'C:\\\\Program Files\\\\mingw-w64\\\\x86_64-7.2.0-posix-seh-rt_v5-rev1\\\\mingw64\\\\bin'\n",
    "# os.environ['PATH'] = mingw_path + ';' + os.environ['PATH']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load UCI ML Breast Cancer Wisconsin (Diagnostic) Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Loading the Breast Cancer Wisconsin dataset\n",
      "--------------------------------------------------\n",
      "Breast Cancer dataset excerpt:\n",
      "\n",
      "         0  1      2      3       4       5        6        7       8   \\\n",
      "0    842302  M  17.99  10.38  122.80  1001.0  0.11840  0.27760  0.3001   \n",
      "1    842517  M  20.57  17.77  132.90  1326.0  0.08474  0.07864  0.0869   \n",
      "2  84300903  M  19.69  21.25  130.00  1203.0  0.10960  0.15990  0.1974   \n",
      "3  84348301  M  11.42  20.38   77.58   386.1  0.14250  0.28390  0.2414   \n",
      "4  84358402  M  20.29  14.34  135.10  1297.0  0.10030  0.13280  0.1980   \n",
      "\n",
      "        9    ...        22     23      24      25      26      27      28  \\\n",
      "0  0.14710   ...     25.38  17.33  184.60  2019.0  0.1622  0.6656  0.7119   \n",
      "1  0.07017   ...     24.99  23.41  158.80  1956.0  0.1238  0.1866  0.2416   \n",
      "2  0.12790   ...     23.57  25.53  152.50  1709.0  0.1444  0.4245  0.4504   \n",
      "3  0.10520   ...     14.91  26.50   98.87   567.7  0.2098  0.8663  0.6869   \n",
      "4  0.10430   ...     22.54  16.67  152.20  1575.0  0.1374  0.2050  0.4000   \n",
      "\n",
      "       29      30       31  \n",
      "0  0.2654  0.4601  0.11890  \n",
      "1  0.1860  0.2750  0.08902  \n",
      "2  0.2430  0.3613  0.08758  \n",
      "3  0.2575  0.6638  0.17300  \n",
      "4  0.1625  0.2364  0.07678  \n",
      "\n",
      "[5 rows x 32 columns] \n",
      "\n",
      "\n",
      "Breast Cancer dataset dimensions:\n",
      "\n",
      "(569, 32)\n"
     ]
    }
   ],
   "source": [
    "print(50 * '=')\n",
    "print('Loading the Breast Cancer Wisconsin dataset')\n",
    "print(50 * '-')\n",
    "breast_cancer_data = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases'\n",
    "                 '/breast-cancer-wisconsin/wdbc.data', header=None)\n",
    "\n",
    "print('Breast Cancer dataset excerpt:\\n')\n",
    "print(breast_cancer_data.head(), '\\n\\n')\n",
    "\n",
    "print('Breast Cancer dataset dimensions:\\n')\n",
    "print(breast_cancer_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = breast_cancer_data.loc[:, 2:].values\n",
    "y = breast_cancer_data.loc[:, 1].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Target Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label encoding example, le.transform(['M', 'B'])\n",
      "[1 0]\n"
     ]
    }
   ],
   "source": [
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)\n",
    "y_enc = le.transform(['M', 'B'])\n",
    "print(\"Label encoding example, le.transform(['M', 'B'])\")\n",
    "print(le.transform(['M', 'B']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Traditional Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.9736842105263158\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Create the training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state=123)\n",
    "\n",
    "# Instantiate the classifier: dt_clf_4\n",
    "dt_clf_4 = DecisionTreeClassifier(max_depth = 4)\n",
    "\n",
    "# Fit the classifier to the training set\n",
    "dt_clf_4.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels of the test set: y_pred_4\n",
    "y_pred_4 = dt_clf_4.predict(X_test)\n",
    "\n",
    "# Compute the accuracy of the predictions: accuracy\n",
    "accuracy = float(np.sum(y_pred_4==y_test))/y_test.shape[0]\n",
    "print(\"Test Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pipelined Logistic Regression Classifier with PCA and Standardization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Section: Combining transformers and estimators in a pipeline\n",
      "--------------------------------------------------\n",
      "Test Accuracy: 0.965\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Create the training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state=123)\n",
    "\n",
    "print(50 * '=')\n",
    "print('Section: Combining transformers and estimators in a pipeline')\n",
    "print(50 * '-')\n",
    "\n",
    "\n",
    "pipe_lr = Pipeline([('scl', StandardScaler()),\n",
    "                    ('pca', PCA(n_components=2)),\n",
    "                    ('clf', LogisticRegression(random_state=1))])\n",
    "\n",
    "pipe_lr.fit(X_train, y_train)\n",
    "print('Test Accuracy: %.3f' % pipe_lr.score(X_test, y_test))\n",
    "y_pred = pipe_lr.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost Native Cross-validation (Accuracy) \n",
    "Use of __xgb.cv__, XGBoost's learning API through its baked-in cross-validation capabilities. XGBoost gets its lauded performance and efficiency gains by utilizing its own optimized data structure for datasets called a DMatrix.\n",
    "\n",
    "In XGBoost: Fit/Predict, the input datasets were converted into DMatrix data on the fly, but when using the xgboost cv object, first explicitly convert data into a DMatrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load data (may have to re-point once in github)\n",
    "churn_data = pd.read_csv(\"https://s3.amazonaws.com/assets.datacamp.com/production/course_3739/datasets/taxi_churn_data_dummified.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   test-error-mean  test-error-std  train-error-mean  train-error-std\n",
      "0          0.28378        0.001932           0.28232         0.002366\n",
      "1          0.27190        0.001932           0.26951         0.001855\n",
      "2          0.25798        0.003963           0.25605         0.003213\n",
      "3          0.25434        0.003827           0.25090         0.001845\n",
      "4          0.24852        0.000934           0.24654         0.001981\n",
      "\n",
      " Accuracy (unpreprocessed) with XGBoost: 0.751480\n"
     ]
    }
   ],
   "source": [
    "# Create the DMatrix: churn_dmatrix\n",
    "churn_dmatrix = xgb.DMatrix(data = churn_data.iloc[:,:-1], label=churn_data.month_5_still_here)\n",
    "\n",
    "# Create the parameter dictionary: params\n",
    "params = {\"objective\":\"reg:logistic\", \"max_depth\":3}\n",
    "\n",
    "# Perform cross-validation: cv_results\n",
    "cv_results = xgb.cv(dtrain=churn_dmatrix, params=params, nfold=3, num_boost_round=5, metrics=\"error\", as_pandas=True, seed=123)\n",
    "\n",
    "# Print cv_results\n",
    "print(cv_results)\n",
    "\n",
    "# Print the accuracy\n",
    "print ('\\n Accuracy (unpreprocessed) with XGBoost: %f' % (((1-cv_results[\"test-error-mean\"]).iloc[-1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cv_results stores the training and test mean and standard deviation of the error per boosting round (tree built) as a DataFrame. From cv_results, the final round 'test-error-mean' is extracted and converted into an accuracy, where accuracy is 1-error. The final accuracy of around 75% is similar to the results determined earlier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost Native Area Under the Curve (AUC)\n",
    "Having used cross-validation to compute average out-of-sample accuracy (after converting from an error), it's very easy to compute any other metric you might be interested in. Simply pass it (or a list of metrics) in as an argument to the metrics parameter of xgb.cv().\n",
    "\n",
    "This exercise will compute another common metric used in binary classification - the area under the curve (\"auc\"). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   test-auc-mean  test-auc-std  train-auc-mean  train-auc-std\n",
      "0       0.767863      0.002820        0.768893       0.001544\n",
      "1       0.789157      0.006846        0.790864       0.006758\n",
      "2       0.814476      0.005997        0.815872       0.003900\n",
      "3       0.821682      0.003912        0.822959       0.002018\n",
      "4       0.826191      0.001937        0.827528       0.000769\n",
      "\n",
      " AUC (unpreprocessed) with XGBoost: 0.826191\n"
     ]
    }
   ],
   "source": [
    "# Perform cross_validation using cv_results and AUC as the metric \n",
    "cv_results = xgb.cv(dtrain=churn_dmatrix, params=params, nfold=3, num_boost_round=5, metrics=\"auc\", as_pandas=True, seed=123)\n",
    "\n",
    "# Print cv_results\n",
    "print(cv_results)\n",
    "\n",
    "# Print the AUC\n",
    "print ('\\n AUC (unpreprocessed) with XGBoost: %f' % (cv_results[\"test-auc-mean\"]).iloc[-1])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
