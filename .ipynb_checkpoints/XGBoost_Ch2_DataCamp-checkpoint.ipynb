{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 2: XGBoost (DataCamp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Course notes from [DataCamp](https://campus.datacamp.com/courses/extreme-gradient-boosting-with-xgboost) XGBoost<br>\n",
    "Importing xgboost requires some [work](https://www.ibm.com/developerworks/community/blogs/jfp/entry/Installing_XGBoost_For_Anaconda_on_Windows?lang=en)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison of Linear and DT Base Learners for XGBoost Regressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear Base Learner:\n",
    "- Sum of linear terms \n",
    "- Boosted model is weighted sum of linear models (thus is itself linear) \n",
    "- Rarely used \n",
    "\n",
    "Tree Base Learner:\n",
    "- Decision tree \n",
    "- Boosted model is weighted sum of decision trees (nonlinear) \n",
    "- Almost exclusively used in XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost Regressor using Decision Trees as Base Learners\n",
    "Build an XGBoost model to predict house prices in Ames, Iowa. \n",
    "\n",
    "In this exercise, the goal is to use trees as base learners. By default, XGBoost uses trees as base learners, so no need to specify trees with booster=\"gbtree\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# may be required as xgboost import throws errors \n",
    "# import os\n",
    "# mingw_path = 'C:\\\\Program Files\\\\mingw-w64\\\\x86_64-7.2.0-posix-seh-rt_v5-rev1\\\\mingw64\\\\bin'\n",
    "# os.environ['PATH'] = mingw_path + ';' + os.environ['PATH']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load data Ames, Iowa dataset from DataCamp's AWS url\n",
    "housing_data = pd.read_csv(\"https://s3.amazonaws.com/assets.datacamp.com/production/course_3786/datasets/ames_housing_trimmed_processed.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create df for the features and the target: X, y\n",
    "X, y = housing_data.iloc[:,:-1], housing_data.iloc[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DT Boosted Linear Regressor RMSE: 78847.401758\n"
     ]
    }
   ],
   "source": [
    "# Create the training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 123)\n",
    "\n",
    "# Instantiate the XGBRegressor: xg_reg\n",
    "xg_reg = xgb.XGBRegressor(n_estimators = 10, objective = 'reg:linear', booster = 'gbtree', seed = 123)\n",
    "\n",
    "# Fit the regressor to the training set\n",
    "xg_reg.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels of the test set: preds\n",
    "preds = xg_reg.predict(X_test)\n",
    "                      \n",
    "# Compute the rmse: rmse\n",
    "rmse = np.sqrt(mean_squared_error(y_test,preds))\n",
    "\n",
    "print(\"DT Boosted Linear Regressor RMSE: %f\" % (rmse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost Regressor using Linear Regression as Base Learner\n",
    "\n",
    "This model, although not as commonly used in XGBoost, allows one to create a regularized linear regression using XGBoost's powerful learning API. However, because it's uncommon, one has to use XGBoost's own non-scikit-learn compatible functions to build the model, such as xgb.train().\n",
    "\n",
    "In order to do this one must create the parameter dictionary that describes the kind of booster one wants to use (similarly creating the dictionary in Chapter 1 with xgb.cv()). The key-value pair that defines the booster type (base model) needed is \"booster\":\"gblinear\".\n",
    "\n",
    "Once model is created, .fit() and .predict() methods of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# may be required as xgboost import throws errors \n",
    "# import os\n",
    "# mingw_path = 'C:\\\\Program Files\\\\mingw-w64\\\\x86_64-7.2.0-posix-seh-rt_v5-rev1\\\\mingw64\\\\bin'\n",
    "# os.environ['PATH'] = mingw_path + ';' + os.environ['PATH']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load data Ames, Iowa dataset from DataCamp's AWS url\n",
    "housing_data = pd.read_csv(\"https://s3.amazonaws.com/assets.datacamp.com/production/course_3786/datasets/ames_housing_trimmed_processed.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create df for the features and the target: X, y\n",
    "\n",
    "X, y = housing_data.iloc[:,:-1], housing_data.iloc[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Boosted Linear Regression RMSE: 40719.741641\n"
     ]
    }
   ],
   "source": [
    "# Convert the training and testing sets into DMatrixes: DM_train, DM_test\n",
    "DM_train = xgb.DMatrix(data=X_train,label=y_train)\n",
    "DM_test =  xgb.DMatrix(data=X_test,label=y_test)\n",
    "\n",
    "# Create the parameter dictionary: params\n",
    "params = {\"booster\":\"gblinear\",\"objective\":\"reg:linear\"}\n",
    "\n",
    "# Train the model: xg_reg\n",
    "xg_reg = xgb.train(params = params, dtrain = DM_train, num_boost_round = 10)\n",
    "\n",
    "# Predict the labels of the test set: preds\n",
    "preds = xg_reg.predict(DM_test)\n",
    "\n",
    "# Compute and print the RMSE\n",
    "rmse = np.sqrt(mean_squared_error(y_test,preds))\n",
    "print(\"Linear Boosted Linear Regression RMSE: %f\" % (rmse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating Model Quality using Root Mean Squared Error (RMSE) and Mean ABS Error (MAE)\n",
    "\n",
    "Compare the RMSE and MAE of a cross-validated XGBoost model on the housing data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   test-rmse-mean  test-rmse-std  train-rmse-mean  train-rmse-std\n",
      "0   142980.433594    1193.791602    141767.531250      429.454591\n",
      "1   104891.394532    1223.158855    102832.544922      322.469930\n",
      "2    79478.937500    1601.344539     75872.615235      266.475960\n",
      "3    62411.920899    2220.150028     57245.652343      273.625086\n",
      "4    51348.279297    2963.377719     44401.298828      316.423666\n",
      "\n",
      " Non-boosted Linear Regression RMSE: 51348.279297\n"
     ]
    }
   ],
   "source": [
    "# RMSE\n",
    "# Create the DMatrix: housing_dmatrix\n",
    "housing_dmatrix = xgb.DMatrix(data=X, label=y)\n",
    "\n",
    "# Create the parameter dictionary: params\n",
    "params = {\"objective\":\"reg:linear\", \"max_depth\":4}\n",
    "\n",
    "# Perform cross-validation: cv_results\n",
    "cv_results = xgb.cv(dtrain=housing_dmatrix, params=params, nfold=4, num_boost_round=5, metrics='rmse', as_pandas=True, seed=123)\n",
    "\n",
    "# Print cv_results\n",
    "print(cv_results)\n",
    "\n",
    "# Extract and print final boosting round metric\n",
    "print(\"\\n Non-boosted Linear Regression RMSE: %f\" % ((cv_results[\"test-rmse-mean\"]).tail(1)))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   test-mae-mean  test-mae-std  train-mae-mean  train-mae-std\n",
      "0  127634.000000   2404.009898   127343.482421     668.308109\n",
      "1   90122.501953   2107.912810    89770.056641     456.965267\n",
      "2   64278.558594   1887.567576    63580.791016     263.404950\n",
      "3   46819.168945   1459.818607    45633.155274     151.883420\n",
      "4   35670.646484   1140.607452    33587.090820      86.999396\n",
      "\n",
      " Non-boosted Linear Regression MAE: 35670.646484\n"
     ]
    }
   ],
   "source": [
    "# MAE\n",
    "# Create the DMatrix: housing_dmatrix\n",
    "housing_dmatrix = xgb.DMatrix(data=X, label=y)\n",
    "\n",
    "# Create the parameter dictionary: params\n",
    "params = {\"objective\":\"reg:linear\", \"max_depth\":4}\n",
    "\n",
    "# Perform cross-validation: cv_results\n",
    "cv_results = xgb.cv(dtrain=housing_dmatrix, params=params, nfold=4, num_boost_round=5, metrics='mae', as_pandas=True, seed=123)\n",
    "\n",
    "# Print cv_results\n",
    "print(cv_results)\n",
    "\n",
    "# Extract and print final boosting round metric\n",
    "\n",
    "print(\"\\n Non-boosted Linear Regression MAE: %f\" % ((cv_results[\"test-mae-mean\"]).tail(1))) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
