{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization and base learners in XGBoost\n",
    "Regularization is a very important technique in machine learning to prevent overfitting. Mathematically speaking, it adds a regularization term in order to prevent the coefficients to fit so perfectly to overfit. The difference between the L1 and L2 is just that L2 is the sum of the square of the weights, while L1 is just the sum of the weights. \n",
    "\n",
    "[Regularization](http://enhancedatascience.com/2017/07/04/machine-learning-explained-regularization/), in the context of machine learning, refers to the process of modifying a learning algorithm so as to prevent overfitting. This generally involves imposing some sort of smoothness constraint on the learned model. This smoothness may be enforced explicitly, by fixing the number of parameters in the model, or by augmenting the cost function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# may be required as xgboost import throws errors \n",
    "# import os\n",
    "# mingw_path = 'C:\\\\Program Files\\\\mingw-w64\\\\x86_64-7.2.0-posix-seh-rt_v5-rev1\\\\mingw64\\\\bin'\n",
    "# os.environ['PATH'] = mingw_path + ';' + os.environ['PATH']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load data Ames, Iowa dataset from DataCamp's AWS url\n",
    "housing_data = pd.read_csv(\"https://s3.amazonaws.com/assets.datacamp.com/production/course_3786/datasets/ames_housing_trimmed_processed.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create df for the features and the target: X, y\n",
    "X, y = housing_data.iloc[:,:-1], housing_data.iloc[:,-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L1 Regularization Penalty ('alpha')\n",
    "The L1 regularization adds a penalty equal to the sum of the absolute value of the coefficients. The L1 regularization will ___shrink some parameters to zero___. Hence some variables will not play any role in the model, L1 regression can be seen as a way to select features in a model.  A regression model that uses L1 regularization technique is called Lasso Regression.  Lasso  (Least Absolute Shrinkage and Selection Operator) adds “absolute value of magnitude” of coefficient as penalty term to the loss function.  If lambda is zero then we will get back OLS whereas a very large value will make coefficients zero hence it will under-fit.\n",
    "\n",
    "Effect on overall model performance on the Ames housing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best rmse as a function of L1:\n",
      "    L1          rmse\n",
      "0    1  35572.514160\n",
      "1   10  35571.970215\n",
      "2  100  35572.370605\n"
     ]
    }
   ],
   "source": [
    "# Create the DMatrix: housing_dmatrix\n",
    "housing_dmatrix = xgb.DMatrix(data=X, label=y)\n",
    "\n",
    "reg_params = [1, 10, 100]\n",
    "\n",
    "# Create the initial parameter dictionary for varying l1 strength: params\n",
    "params = {\"objective\":\"reg:linear\",\"max_depth\":4}\n",
    "\n",
    "# Create an empty list for storing rmses as a function of l1 complexity\n",
    "rmses_l1 = []\n",
    "\n",
    "# Iterate over reg_params\n",
    "for reg in reg_params:\n",
    "\n",
    "    # Update l1 strength\n",
    "    params[\"alpha\"] = reg\n",
    "    \n",
    "    # Pass this updated param dictionary into cv\n",
    "    cv_results_rmse = xgb.cv(dtrain=housing_dmatrix, params=params, nfold=4, \n",
    "                             num_boost_round=10, metrics=\"rmse\", as_pandas=True, seed=123)\n",
    "    \n",
    "    # Append best rmse (final round) to rmses_l1\n",
    "    rmses_l1.append(cv_results_rmse[\"test-rmse-mean\"].tail(1).values[0])\n",
    "\n",
    "# Look at best rmse per l1 param\n",
    "print(\"Best rmse as a function of L1:\")\n",
    "print(pd.DataFrame(list(zip(reg_params, rmses_l1)), columns=[\"L1\", \"rmse\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observation: alpha (L1 regularization) has little to no effect on RMSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L2 Regularization Penalty ('lambda')\n",
    "A regression model that uses L2 is called Ridge Regression.\n",
    "Ridge regression adds “squared magnitude” of coefficient as penalty term to the loss function.  The L2 regularization adds a penalty equal to the sum of the squared value of the coefficients. The L2 regularization will force the parameters to be relatively small; the bigger the penalization, the smaller (and the more robust) the coefficients.  \n",
    "\n",
    "Effect on overall model performance on the Ames housing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best rmse as a function of l2:\n",
      "    l2          rmse\n",
      "0    1  52275.357421\n",
      "1   10  57746.064453\n",
      "2  100  76624.625000\n"
     ]
    }
   ],
   "source": [
    "# Create the DMatrix: housing_dmatrix\n",
    "housing_dmatrix = xgb.DMatrix(data=X, label=y)\n",
    "\n",
    "reg_params = [1, 10, 100]\n",
    "\n",
    "# Create the initial parameter dictionary for varying l2 strength: params\n",
    "params = {\"objective\":\"reg:linear\",\"max_depth\":3}\n",
    "\n",
    "# Create an empty list for storing rmses as a function of l2 complexity\n",
    "rmses_l2 = []\n",
    "\n",
    "# Iterate over reg_params\n",
    "for reg in reg_params:\n",
    "\n",
    "    # Update l2 strength\n",
    "    params[\"lambda\"] = reg\n",
    "    \n",
    "    # Pass this updated param dictionary into cv\n",
    "    cv_results_rmse = xgb.cv(dtrain=housing_dmatrix, params=params, nfold=2, \n",
    "                             num_boost_round=5, metrics=\"rmse\", as_pandas=True, seed=123)\n",
    "    \n",
    "    # Append best rmse (final round) to rmses_l2\n",
    "    rmses_l2.append(cv_results_rmse[\"test-rmse-mean\"].tail(1).values[0])\n",
    "\n",
    "# Look at best rmse per l2 param\n",
    "print(\"Best rmse as a function of l2:\")\n",
    "print(pd.DataFrame(list(zip(reg_params, rmses_l2)), columns=[\"l2\", \"rmse\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observation: RMSE increases with lambda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The key difference between these techniques is that Lasso shrinks the less important feature’s coefficient to zero thus, removing some feature altogether. So, this works well for feature selection in case we have a huge number of features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Base Learner: \n",
    "    - Sum of linear terms \n",
    "    - Boosted model is weighted sum of linear models (thus is itself linear) \n",
    "    - Rarely used \n",
    "### Tree Base Learner: \n",
    "    - Decision tree \n",
    "    - Boosted model is weighted sum of decision trees (nonlinear) \n",
    "    - Almost exclusively used in XGBoost"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
