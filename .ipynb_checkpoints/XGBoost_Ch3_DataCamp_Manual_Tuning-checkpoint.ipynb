{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning Parameters Manually"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Common Tree Tunable Parameters\n",
    "    -learning rate: learning rate/eta (how quickly model fits residual error using additional base learners)\n",
    "    -gamma: min loss reduction to create new tree split\n",
    "    -lambda: L2 reg on leaf weights\n",
    "    -alpha: L1 reg on leaf weights\n",
    "    -max_depth: max depth per tree (how deep can tree grow)\n",
    "    -subsample: % samples used per tree (0-1: fraction of total training set to be used per boosting round)\n",
    "    -colsample_bytree: % features used per tree (0-1: fraction of total features to be used per boosting round)\n",
    "    -number of estimators (base learners)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Common Linear Tunable Parameters\n",
    "    -lambda: L2 reg on weights\n",
    "    -alpha: L1 reg on weights\n",
    "    -lambda_bias: L2 reg term on bias\n",
    "    -number of estimators (base learners)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Untuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Untuned rmse: 34624.229980\n"
     ]
    }
   ],
   "source": [
    "# load Ames housing dataset\n",
    "housing_data = pd.read_csv(\"https://s3.amazonaws.com/assets.datacamp.com/production/course_3786/datasets/ames_housing_trimmed_processed.csv\")\n",
    "\n",
    "# build train and target dataframes\n",
    "X,y = housing_data[housing_data.columns.tolist()[:-1]], housing_data[housing_data.columns.tolist()[-1]]\n",
    "\n",
    "housing_dmatrix = xgb.DMatrix(data=X,label=y)\n",
    "\n",
    "untuned_params={\"objective\":\"reg:linear\"}\n",
    "\n",
    "untuned_cv_results_rmse = xgb.cv(dtrain=housing_dmatrix, \n",
    "                                 params=untuned_params, nfold=4, metrics=\"rmse\", as_pandas=True, seed=123)\n",
    "\n",
    "print(\"Untuned rmse: %f\" %((untuned_cv_results_rmse[\"test-rmse-mean\"]).tail(1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuned Model (colsample, learning_rate, max_depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuned rmse: 29812.683594\n"
     ]
    }
   ],
   "source": [
    "# load Ames housing dataset\n",
    "housing_data = pd.read_csv(\"https://s3.amazonaws.com/assets.datacamp.com/production/course_3786/datasets/ames_housing_trimmed_processed.csv\")\n",
    "\n",
    "# build train and target dataframes\n",
    "X,y = housing_data[housing_data.columns.tolist()[:-1]], housing_data[housing_data.columns.tolist()[-1]]\n",
    "\n",
    "housing_dmatrix = xgb.DMatrix(data=X,label=y)\n",
    "\n",
    "tuned_params = {\"objective\":\"reg:linear\",'colsample_bytree': 0.3,'learning_rate': 0.1,'max_depth': 5}\n",
    "\n",
    "tuned_cv_results_rmse = xgb.cv(dtrain=housing_dmatrix, params=tuned_params, nfold=4, \n",
    "                                       num_boost_round=200, metrics=\"rmse\", as_pandas=True, seed=123)\n",
    "\n",
    "print(\"Tuned rmse: %f\" %((tuned_cv_results_rmse[\"test-rmse-mean\"]).tail(1)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observation: Tuned RMSE of 29,812 compared to untuned RMSE of 34,624"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning the number of boosting rounds\n",
    "#### Explore effect of boosting rounds (number of trees built) on the out-of-sample performance of  XGBoost model. \n",
    "Schema:  cherry pick with a for loop - use xgb.cv() inside a for loop and build one model per num_boost_round parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   num_boosting_rounds          rmse\n",
      "0                    5  50903.299479\n",
      "1                   10  34774.194010\n",
      "2                   15  32895.098958\n"
     ]
    }
   ],
   "source": [
    "# load Ames housing dataset\n",
    "housing_data = pd.read_csv(\"https://s3.amazonaws.com/assets.datacamp.com/production/course_3786/datasets/ames_housing_trimmed_processed.csv\")\n",
    "\n",
    "# build train and target dataframes\n",
    "X,y = housing_data[housing_data.columns.tolist()[:-1]], housing_data[housing_data.columns.tolist()[-1]]\n",
    "\n",
    "# Create the DMatrix: housing_dmatrix\n",
    "housing_dmatrix = xgb.DMatrix(data=X,label=y)\n",
    "\n",
    "# Create the parameter dictionary for each tree: params \n",
    "params = {\"objective\":\"reg:linear\", 'max_depth': 3}\n",
    "\n",
    "# Create list of number of boosting rounds\n",
    "num_rounds = [5, 10, 15]\n",
    "\n",
    "# Empty list to store final round rmse per XGBoost model\n",
    "final_rmse_per_round = []\n",
    "\n",
    "# Iterate over num_rounds and build one model per num_boost_round parameter\n",
    "for curr_num_rounds in num_rounds:\n",
    "\n",
    "    # Perform cross-validation: cv_results\n",
    "    cv_results = xgb.cv(dtrain = housing_dmatrix, params = params, nfold = 3, \n",
    "                        num_boost_round = curr_num_rounds, metrics=\"rmse\", as_pandas=True, seed=123)\n",
    "    \n",
    "    # Append final round RMSE\n",
    "    final_rmse_per_round.append(cv_results[\"test-rmse-mean\"].tail().values[-1])\n",
    "\n",
    "# Print the resultant DataFrame\n",
    "num_rounds_rmses = list(zip(num_rounds, final_rmse_per_round))\n",
    "\n",
    "print(pd.DataFrame(num_rounds_rmses,columns=[\"num_boosting_rounds\",\"rmse\"]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### Observation: Increase in boosting rounds (number of trees built) results in a non-linear increase in out of sample performance\n",
    " |  num_boosting_rounds | rmse \n",
    "|---|---|\n",
    "| 5  |50903.299479  \n",
    "|  10 |34774.194010  \n",
    "| 15  |32895.098958   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automated boosting round selection using early_stopping\n",
    "Now, instead of attempting to cherry pick the best possible number of boosting rounds, rely on XGBoost to automatically select the number of boosting rounds within xgb.cv(). This is done using a technique called __early stopping__.\n",
    "\n",
    "Early stopping works by testing the XGBoost model after every boosting round against a hold-out dataset and stopping the creation of additional boosting rounds (thereby finishing training of the model early) if the hold-out metric (\"rmse\" in this case) does not improve for a given number of rounds. \n",
    "\n",
    "The __early_stopping_rounds__ parameter in xgb.cv() will use a large number of boosting rounds (50) (trees). Bear in mind that if the holdout metric continuously improves up through when num_boosting_rounds is reached, then early stopping does not occur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    test-rmse-mean  test-rmse-std  train-rmse-mean  train-rmse-std\n",
      "0    142640.656250     705.559400    141871.630208      403.632626\n",
      "1    104907.664063     111.113862    103057.036458       73.769561\n",
      "2     79262.059895     563.766991     75975.966146      253.726099\n",
      "3     61620.136719    1087.694282     57420.529948      521.658354\n",
      "4     50437.562500    1846.448017     44552.955729      544.170190\n",
      "5     43035.658854    2034.471024     35763.949219      681.798925\n",
      "6     38600.880208    2169.796232     29861.464844      769.571318\n",
      "7     36071.817708    2109.795430     25994.675781      756.521419\n",
      "8     34383.184896    1934.546688     23306.836588      759.238254\n",
      "9     33509.139974    1887.375633     21459.770833      745.624404\n",
      "10    32916.805990    1850.893363     20148.721354      749.612769\n",
      "11    32197.832682    1734.456935     19215.382813      641.387376\n",
      "12    31770.852865    1802.155484     18627.389323      716.256596\n",
      "13    31482.782552    1779.123767     17960.695312      557.043568\n",
      "14    31389.990234    1892.319927     17559.736979      631.412969\n",
      "15    31302.883464    1955.166046     17205.712891      590.171393\n",
      "16    31234.058594    1880.705796     16876.571940      703.631755\n",
      "17    31318.347656    1828.860164     16597.662110      703.677609\n",
      "18    31323.634766    1775.909567     16330.460937      607.274494\n",
      "19    31204.135417    1739.076156     16005.972982      520.470911\n",
      "20    31089.863932    1756.022575     15814.300781      518.604760\n",
      "21    31047.998047    1624.672407     15493.405924      505.616658\n",
      "22    31056.916667    1668.043013     15270.734375      502.018453\n",
      "23    31024.983724    1548.985354     15086.382162      503.913199\n",
      "24    30983.685547    1663.130510     14917.608399      486.206187\n",
      "25    30989.477214    1686.668050     14709.589518      449.668010\n",
      "26    30952.113932    1613.172643     14457.286133      376.787666\n",
      "27    31066.902344    1648.534310     14185.567057      383.102691\n",
      "28    31095.642578    1709.225327     13934.066732      473.465449\n",
      "29    31103.887370    1778.880069     13749.644857      473.670886\n",
      "30    30976.084635    1744.514164     13549.836589      454.898834\n",
      "31    30938.469401    1746.052597     13413.484700      399.603323\n",
      "32    30931.000000    1772.469510     13275.915364      415.408340\n",
      "33    30929.056641    1765.541578     13085.878255      493.792778\n",
      "34    30890.629557    1786.510976     12947.181315      517.790039\n",
      "35    30884.493490    1769.729143     12846.027344      547.732372\n",
      "36    30833.542318    1691.001567     12702.378581      505.523315\n",
      "37    30856.688151    1771.445059     12532.243815      508.298162\n",
      "38    30818.016927    1782.784630     12384.055013      536.225042\n",
      "39    30839.392578    1847.327022     12198.443359      545.165562\n",
      "40    30776.964844    1912.781000     12054.583659      508.841772\n",
      "41    30794.702474    1919.674832     11897.036458      477.177568\n",
      "42    30780.956380    1906.820987     11756.221354      502.992395\n",
      "43    30783.754557    1951.259784     11618.846680      519.837469\n",
      "44    30776.731120    1953.447693     11484.080404      578.428621\n",
      "45    30758.543620    1947.454953     11356.552734      565.368794\n",
      "46    30729.971354    1985.698867     11193.557943      552.299272\n",
      "47    30732.662760    1966.997355     11071.315755      604.090310\n",
      "48    30712.241536    1957.751573     10950.778320      574.862779\n",
      "49    30720.854167    1950.511057     10824.865560      576.665674\n",
      "Early stopped rmse: 30720.854167\n"
     ]
    }
   ],
   "source": [
    "# load Ames housing dataset\n",
    "housing_data = pd.read_csv(\"https://s3.amazonaws.com/assets.datacamp.com/production/course_3786/datasets/ames_housing_trimmed_processed.csv\")\n",
    "\n",
    "# build train and target dataframes\n",
    "X,y = housing_data[housing_data.columns.tolist()[:-1]], housing_data[housing_data.columns.tolist()[-1]]\n",
    "\n",
    "# Create your housing DMatrix: housing_dmatrix\n",
    "housing_dmatrix = xgb.DMatrix(data=X, label=y)\n",
    "\n",
    "# Create the parameter dictionary for each tree: params\n",
    "params = {\"objective\":\"reg:linear\", \"max_depth\":4}\n",
    "\n",
    "# Perform cross-validation with early stopping: cv_results\n",
    "cv_results = xgb.cv(dtrain = housing_dmatrix, params = params, nfold = 3, early_stopping_rounds = 5, \n",
    "                    num_boost_round = 50, metrics=\"rmse\", as_pandas=True, seed=123)\n",
    "\n",
    "# Print cv_results\n",
    "print(cv_results)\n",
    "\n",
    "print(\"Early stopped rmse: %f\" %((cv_results[\"test-rmse-mean\"]).tail(1)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning Learning Rate (ETA)\n",
    "The learning rate in XGBoost is a parameter that can range between 0 and 1, with higher values of \"eta\" penalizing feature weights more strongly, causing much stronger regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     eta      best_rmse\n",
      "0  0.001  195736.406250\n",
      "1  0.010  179932.182292\n",
      "2  0.100   79759.411458\n"
     ]
    }
   ],
   "source": [
    "# load Ames housing dataset\n",
    "housing_data = pd.read_csv(\"https://s3.amazonaws.com/assets.datacamp.com/production/course_3786/datasets/ames_housing_trimmed_processed.csv\")\n",
    "\n",
    "# build train and target dataframes\n",
    "X,y = housing_data[housing_data.columns.tolist()[:-1]], housing_data[housing_data.columns.tolist()[-1]]\n",
    "\n",
    "# Create your housing DMatrix: housing_dmatrix\n",
    "housing_dmatrix = xgb.DMatrix(data=X, label=y)\n",
    "\n",
    "# Create the parameter dictionary for each tree (boosting round)\n",
    "params = {\"objective\":\"reg:linear\", \"max_depth\":3}\n",
    "\n",
    "# Create list of eta values and empty list to store final round rmse per xgboost model\n",
    "eta_vals = [0.001, 0.01, 0.1]\n",
    "\n",
    "# Empty list to store best RMSE per XGBoost model\n",
    "best_rmse = []\n",
    "\n",
    "# Systematically vary the eta \n",
    "for curr_val in eta_vals:\n",
    "\n",
    "    params[\"eta\"] = curr_val\n",
    "    \n",
    "    # Perform cross-validation: cv_results\n",
    "    cv_results = xgb.cv(dtrain = housing_dmatrix, params = params, nfold = 3, early_stopping_rounds = 5, \n",
    "                    num_boost_round = 10, metrics=\"rmse\", as_pandas=True, seed=123)\n",
    "        \n",
    "    # Append the final round rmse to best_rmse\n",
    "    best_rmse.append(cv_results[\"test-rmse-mean\"].tail().values[-1])\n",
    "\n",
    "# Print the resultant DataFrame\n",
    "print(pd.DataFrame(list(zip(eta_vals, best_rmse)), columns=[\"eta\",\"best_rmse\"]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  #### Observation: Increase in learning time (how quickly model fits residual error using additional base learners) <br> results in a non-linear increase in out of sample performance\n",
    "\n",
    "    \n",
    "    \n",
    "|  eta | rmse \n",
    "|---|---|\n",
    "| 0.001  |195736.406250 \n",
    "| 0.01 |179932.182292  \n",
    "| 0.1  |79759.411458 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning max_depth\n",
    "The parameter __max_depth__ parameter dictates the maximum depth that each tree in a boosting round can grow to. Smaller values will lead to shallower trees, and larger values to deeper trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   max_depth     best_rmse\n",
      "0          2  37957.468750\n",
      "1          5  35596.599610\n",
      "2         10  36065.546875\n",
      "3         20  36739.576172\n"
     ]
    }
   ],
   "source": [
    "# load Ames housing dataset\n",
    "housing_data = pd.read_csv(\"https://s3.amazonaws.com/assets.datacamp.com/production/course_3786/datasets/ames_housing_trimmed_processed.csv\")\n",
    "\n",
    "# build train and target dataframes\n",
    "X,y = housing_data[housing_data.columns.tolist()[:-1]], housing_data[housing_data.columns.tolist()[-1]]\n",
    "\n",
    "# Create your housing DMatrix\n",
    "housing_dmatrix = xgb.DMatrix(data=X,label=y)\n",
    "\n",
    "# Create the parameter dictionary\n",
    "params = {\"objective\":\"reg:linear\"}\n",
    "\n",
    "# Create list of max_depth values\n",
    "max_depths = [2, 5, 10, 20]\n",
    "\n",
    "# Empty list to store best RMSE per XGBoost model\n",
    "best_rmse = []\n",
    "\n",
    "# Systematically vary the max_depth\n",
    "for curr_val in max_depths:\n",
    "\n",
    "    params[\"max_depth\"] = curr_val\n",
    "    \n",
    "    # Perform cross-validation\n",
    "    cv_results = xgb.cv(dtrain = housing_dmatrix, params = params, nfold = 2, early_stopping_rounds = 5, \n",
    "                    num_boost_round = 10, metrics=\"rmse\", as_pandas=True, seed=123)\n",
    "    \n",
    "    \n",
    "    # Append the final round rmse to best_rmse\n",
    "    best_rmse.append(cv_results[\"test-rmse-mean\"].tail().values[-1])\n",
    "\n",
    "# Print the resultant DataFrame\n",
    "print(pd.DataFrame(list(zip(max_depths, best_rmse)),columns=[\"max_depth\",\"best_rmse\"]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|  max_depth      | best_rmse \n",
    "|---|---|\n",
    "| 2  |37957.468750| \n",
    "| 5 |35596.599610|  \n",
    "| 10  |36065.546875| \n",
    "| 20 | 36739.576172|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observation: increasing tree depth led to overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning colsample_bytree\n",
    "Tune \"colsample_bytree\". This is organic with scikit-learn's RandomForestClassifier or RandomForestRegressor, where it is called __max_features__. \n",
    "\n",
    "In both xgboost and sklearn, this parameter (although named differently) simply specifies the fraction of features to choose from at every split in a given tree. In xgboost, colsample_bytree must be specified as a float between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   colsample_bytree     best_rmse\n",
      "0               0.1  44363.458985\n",
      "1               0.5  36266.462890\n",
      "2               0.8  35704.357422\n",
      "3               1.0  35836.046875\n"
     ]
    }
   ],
   "source": [
    "# load Ames housing dataset\n",
    "housing_data = pd.read_csv(\"https://s3.amazonaws.com/assets.datacamp.com/production/course_3786/datasets/ames_housing_trimmed_processed.csv\")\n",
    "\n",
    "# build train and target dataframes\n",
    "X,y = housing_data[housing_data.columns.tolist()[:-1]], housing_data[housing_data.columns.tolist()[-1]]\n",
    "\n",
    "# Create your housing DMatrix\n",
    "housing_dmatrix = xgb.DMatrix(data=X,label=y)\n",
    "\n",
    "# Create the parameter dictionary\n",
    "params={\"objective\":\"reg:linear\",\"max_depth\":3}\n",
    "\n",
    "# Create list of hyperparameter values: colsample_bytree_vals\n",
    "colsample_bytree_vals = [0.1, 0.5, 0.8, 1]\n",
    "\n",
    "# Empty list to store best RMSE per XGBoost model\n",
    "best_rmse = []\n",
    "\n",
    "# Systematically vary the hyperparameter value \n",
    "for curr_val in colsample_bytree_vals:\n",
    "\n",
    "    params['colsample_bytree'] = curr_val\n",
    "    \n",
    "    # Perform cross-validation\n",
    "    cv_results = xgb.cv(dtrain=housing_dmatrix, params=params, nfold=2,\n",
    "                 num_boost_round=10, early_stopping_rounds=5,\n",
    "                 metrics=\"rmse\", as_pandas=True, seed=123)\n",
    "    \n",
    "    # Append the final round rmse to best_rmse\n",
    "    best_rmse.append(cv_results[\"test-rmse-mean\"].tail().values[-1])\n",
    "\n",
    "# Print the resultant DataFrame\n",
    "print(pd.DataFrame(list(zip(colsample_bytree_vals, best_rmse)), columns=[\"colsample_bytree\",\"best_rmse\"]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observation: trees using 80% of features were most performant\n",
    "\n",
    "|  colsample_bytree| best_rmse |\n",
    "|---|---|\n",
    "| 0.1  |44363.458985| \n",
    "| 0.5 |36266.462890|  \n",
    "| 0.8  |35704.357422| \n",
    "| 1.0 | 35836.046875|"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
