{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding Categorical Features\n",
    "True practice of data science will require preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load data Ames, Iowa dataset from DataCamp's AWS url\n",
    "ames_housing_data = pd.read_csv(\"https://s3.amazonaws.com/assets.datacamp.com/production/course_3970/datasets/ames_unprocessed_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "columns_rearranged = [\"MSSubClass\",\"LotFrontage\",\"LotArea\"]+ames_housing_data.columns.tolist()[7:-2]+[\"MSZoning\",\"PavedDrive\"]+ames_housing_data.columns.tolist()[4:7]+[\"SalePrice\"]\n",
    "ames_housing_data = ames_housing_data[columns_rearranged]\n",
    "X,y = ames_housing_data.iloc[:,:-1],ames_housing_data.iloc[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1460, 21)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ames_housing_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MSSubClass</th>\n",
       "      <th>LotFrontage</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>OverallQual</th>\n",
       "      <th>OverallCond</th>\n",
       "      <th>YearBuilt</th>\n",
       "      <th>Remodeled</th>\n",
       "      <th>GrLivArea</th>\n",
       "      <th>BsmtFullBath</th>\n",
       "      <th>BsmtHalfBath</th>\n",
       "      <th>FullBath</th>\n",
       "      <th>HalfBath</th>\n",
       "      <th>BedroomAbvGr</th>\n",
       "      <th>Fireplaces</th>\n",
       "      <th>GarageArea</th>\n",
       "      <th>SalePrice</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1460.000000</td>\n",
       "      <td>1201.000000</td>\n",
       "      <td>1460.000000</td>\n",
       "      <td>1460.000000</td>\n",
       "      <td>1460.000000</td>\n",
       "      <td>1460.000000</td>\n",
       "      <td>1460.000000</td>\n",
       "      <td>1460.000000</td>\n",
       "      <td>1460.000000</td>\n",
       "      <td>1460.000000</td>\n",
       "      <td>1460.000000</td>\n",
       "      <td>1460.000000</td>\n",
       "      <td>1460.000000</td>\n",
       "      <td>1460.000000</td>\n",
       "      <td>1460.000000</td>\n",
       "      <td>1460.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>56.897260</td>\n",
       "      <td>70.049958</td>\n",
       "      <td>10516.828082</td>\n",
       "      <td>6.099315</td>\n",
       "      <td>5.575342</td>\n",
       "      <td>1971.267808</td>\n",
       "      <td>0.476712</td>\n",
       "      <td>1515.463699</td>\n",
       "      <td>0.425342</td>\n",
       "      <td>0.057534</td>\n",
       "      <td>1.565068</td>\n",
       "      <td>0.382877</td>\n",
       "      <td>2.866438</td>\n",
       "      <td>0.613014</td>\n",
       "      <td>472.980137</td>\n",
       "      <td>180921.195890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>42.300571</td>\n",
       "      <td>24.284752</td>\n",
       "      <td>9981.264932</td>\n",
       "      <td>1.382997</td>\n",
       "      <td>1.112799</td>\n",
       "      <td>30.202904</td>\n",
       "      <td>0.499629</td>\n",
       "      <td>525.480383</td>\n",
       "      <td>0.518911</td>\n",
       "      <td>0.238753</td>\n",
       "      <td>0.550916</td>\n",
       "      <td>0.502885</td>\n",
       "      <td>0.815778</td>\n",
       "      <td>0.644666</td>\n",
       "      <td>213.804841</td>\n",
       "      <td>79442.502883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>20.000000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>1300.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1872.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>334.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>34900.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>20.000000</td>\n",
       "      <td>59.000000</td>\n",
       "      <td>7553.500000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>1954.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1129.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>334.500000</td>\n",
       "      <td>129975.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>50.000000</td>\n",
       "      <td>69.000000</td>\n",
       "      <td>9478.500000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>1973.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1464.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>480.000000</td>\n",
       "      <td>163000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>70.000000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>11601.500000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1776.750000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>576.000000</td>\n",
       "      <td>214000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>190.000000</td>\n",
       "      <td>313.000000</td>\n",
       "      <td>215245.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>2010.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>5642.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1418.000000</td>\n",
       "      <td>755000.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        MSSubClass  LotFrontage        LotArea  OverallQual  OverallCond  \\\n",
       "count  1460.000000  1201.000000    1460.000000  1460.000000  1460.000000   \n",
       "mean     56.897260    70.049958   10516.828082     6.099315     5.575342   \n",
       "std      42.300571    24.284752    9981.264932     1.382997     1.112799   \n",
       "min      20.000000    21.000000    1300.000000     1.000000     1.000000   \n",
       "25%      20.000000    59.000000    7553.500000     5.000000     5.000000   \n",
       "50%      50.000000    69.000000    9478.500000     6.000000     5.000000   \n",
       "75%      70.000000    80.000000   11601.500000     7.000000     6.000000   \n",
       "max     190.000000   313.000000  215245.000000    10.000000     9.000000   \n",
       "\n",
       "         YearBuilt    Remodeled    GrLivArea  BsmtFullBath  BsmtHalfBath  \\\n",
       "count  1460.000000  1460.000000  1460.000000   1460.000000   1460.000000   \n",
       "mean   1971.267808     0.476712  1515.463699      0.425342      0.057534   \n",
       "std      30.202904     0.499629   525.480383      0.518911      0.238753   \n",
       "min    1872.000000     0.000000   334.000000      0.000000      0.000000   \n",
       "25%    1954.000000     0.000000  1129.500000      0.000000      0.000000   \n",
       "50%    1973.000000     0.000000  1464.000000      0.000000      0.000000   \n",
       "75%    2000.000000     1.000000  1776.750000      1.000000      0.000000   \n",
       "max    2010.000000     1.000000  5642.000000      3.000000      2.000000   \n",
       "\n",
       "          FullBath     HalfBath  BedroomAbvGr   Fireplaces   GarageArea  \\\n",
       "count  1460.000000  1460.000000   1460.000000  1460.000000  1460.000000   \n",
       "mean      1.565068     0.382877      2.866438     0.613014   472.980137   \n",
       "std       0.550916     0.502885      0.815778     0.644666   213.804841   \n",
       "min       0.000000     0.000000      0.000000     0.000000     0.000000   \n",
       "25%       1.000000     0.000000      2.000000     0.000000   334.500000   \n",
       "50%       2.000000     0.000000      3.000000     1.000000   480.000000   \n",
       "75%       2.000000     1.000000      3.000000     1.000000   576.000000   \n",
       "max       3.000000     2.000000      8.000000     3.000000  1418.000000   \n",
       "\n",
       "           SalePrice  \n",
       "count    1460.000000  \n",
       "mean   180921.195890  \n",
       "std     79442.502883  \n",
       "min     34900.000000  \n",
       "25%    129975.000000  \n",
       "50%    163000.000000  \n",
       "75%    214000.000000  \n",
       "max    755000.000000  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ames_housing_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 8 entries, count to max\n",
      "Data columns (total 16 columns):\n",
      "MSSubClass      8 non-null float64\n",
      "LotFrontage     8 non-null float64\n",
      "LotArea         8 non-null float64\n",
      "OverallQual     8 non-null float64\n",
      "OverallCond     8 non-null float64\n",
      "YearBuilt       8 non-null float64\n",
      "Remodeled       8 non-null float64\n",
      "GrLivArea       8 non-null float64\n",
      "BsmtFullBath    8 non-null float64\n",
      "BsmtHalfBath    8 non-null float64\n",
      "FullBath        8 non-null float64\n",
      "HalfBath        8 non-null float64\n",
      "BedroomAbvGr    8 non-null float64\n",
      "Fireplaces      8 non-null float64\n",
      "GarageArea      8 non-null float64\n",
      "SalePrice       8 non-null float64\n",
      "dtypes: float64(16)\n",
      "memory usage: 1.1+ KB\n"
     ]
    }
   ],
   "source": [
    "ames_housing_data.describe().info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Observation__: LotFrontage column has 259 missing values.  MSZoning, PavedDrive, and HouseStyle are categorical. These need to be encoded numerically before we can use XGBoost. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  LabelEncoder\n",
    "Encode any categorical columns in the dataset using __ LabelEncoder__ so that they are encoded numerically. <br>\n",
    "The data has five categorical columns: \n",
    "- MSZoning\n",
    "- PavedDrive\n",
    "- Neighborhood\n",
    "- BldgType\n",
    "- HouseStyle. <br> \n",
    "\n",
    "Scikit-learn has a LabelEncoder function that converts the values in each categorical column into integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# required to submit soln in DataCamp shell\n",
    "df = ames_housing_data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Fill missing values with 0\n",
    "df.LotFrontage = df.LotFrontage.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import LabelEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Create a boolean mask for categorical columns\n",
    "categorical_mask = (df.dtypes == object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FullBath        False\n",
       "HalfBath        False\n",
       "BedroomAbvGr    False\n",
       "Fireplaces      False\n",
       "GarageArea      False\n",
       "MSZoning         True\n",
       "PavedDrive       True\n",
       "Neighborhood     True\n",
       "BldgType         True\n",
       "HouseStyle       True\n",
       "SalePrice       False\n",
       "dtype: bool"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categorical_mask[10:25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get list of categorical column names\n",
    "categorical_columns = df.columns[categorical_mask].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['MSZoning', 'PavedDrive', 'Neighborhood', 'BldgType', 'HouseStyle']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categorical_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  MSZoning PavedDrive Neighborhood BldgType HouseStyle\n",
      "0       RL          Y      CollgCr     1Fam     2Story\n",
      "1       RL          Y      Veenker     1Fam     1Story\n",
      "2       RL          Y      CollgCr     1Fam     2Story\n",
      "3       RL          Y      Crawfor     1Fam     2Story\n",
      "4       RL          Y      NoRidge     1Fam     2Story\n"
     ]
    }
   ],
   "source": [
    "# Print the head of the categorical columns\n",
    "print(df[categorical_columns].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create LabelEncoder object: le\n",
    "le = LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Apply LabelEncoder to categorical columns\n",
    "df[categorical_columns] = df[categorical_columns].apply(lambda x: le.fit_transform(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   MSZoning  PavedDrive  Neighborhood  BldgType  HouseStyle\n",
      "0         3           2             5         0           5\n",
      "1         3           2            24         0           2\n",
      "2         3           2             5         0           5\n",
      "3         3           2             6         0           5\n",
      "4         3           2            15         0           5\n"
     ]
    }
   ],
   "source": [
    "# Print the head of the categorical columns\n",
    "print(df[categorical_columns].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OneHotEncoder\n",
    "In the categorical columns of this dataset, there is no natural ordering between the entries. As an example: Using LabelEncoder, the CollgCr Neighborhood was encoded as 5, while the Veenker Neighborhood was encoded as 24, and Crawfor as 6. Is Veenker \"greater\" than Crawfor and CollgCr? No - and allowing the model to assume this natural ordering may result in poor performance.\n",
    "\n",
    "As a result, there is another step needed: Apply a one-hot encoding to create binary, or \"dummy\" variables. Scikit-learn offers OneHotEncoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# required to submit soln in DataCamp shell\n",
    "df = ames_housing_data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Fill missing values with 0\n",
    "df.LotFrontage = df.LotFrontage.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import OneHotEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Create a boolean mask for categorical columns\n",
    "categorical_mask = (df.dtypes == object)\n",
    "\n",
    "# Create OneHotEncoder: ohe\n",
    "ohe = OneHotEncoder(categorical_features=categorical_mask, sparse=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OneHotEncoder(categorical_features=MSSubClass      False\n",
       "LotFrontage     False\n",
       "LotArea         False\n",
       "OverallQual     False\n",
       "OverallCond     False\n",
       "YearBuilt       False\n",
       "Remodeled       False\n",
       "GrLivArea       False\n",
       "BsmtFullBath    False\n",
       "BsmtHalfBath    False\n",
       "FullBath        False\n",
       "HalfBath        False\n",
       "BedroomAbvGr    False\n",
       "Fireplaces      False\n",
       "GarageArea      False\n",
       "MSZoning         True\n",
       "PavedDrive       True\n",
       "Neighborhood     True\n",
       "BldgType         True\n",
       "HouseStyle       True\n",
       "SalePrice       False\n",
       "dtype: bool,\n",
       "       dtype=<class 'numpy.float64'>, handle_unknown='error',\n",
       "       n_values='auto', sparse=False)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ohe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: '1Story'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-44-f43199128245>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Apply OneHotEncoder to categorical columns - output is no longer a dataframe: df_encoded\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdf_encoded\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mohe\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m   2017\u001b[0m         \"\"\"\n\u001b[0;32m   2018\u001b[0m         return _transform_selected(X, self._fit_transform,\n\u001b[1;32m-> 2019\u001b[1;33m                                    self.categorical_features, copy=True)\n\u001b[0m\u001b[0;32m   2020\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2021\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py\u001b[0m in \u001b[0;36m_transform_selected\u001b[1;34m(X, transform, selected, copy)\u001b[0m\n\u001b[0;32m   1807\u001b[0m     \u001b[0mX\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0marray\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0msparse\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_samples\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_features_new\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1808\u001b[0m     \"\"\"\n\u001b[1;32m-> 1809\u001b[1;33m     \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'csc'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mFLOAT_DTYPES\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1810\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1811\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mselected\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstring_types\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mselected\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"all\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[0;32m    431\u001b[0m                                       force_all_finite)\n\u001b[0;32m    432\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 433\u001b[1;33m         \u001b[0marray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    434\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    435\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: could not convert string to float: '1Story'"
     ]
    }
   ],
   "source": [
    "# Apply OneHotEncoder to categorical columns - output is no longer a dataframe: df_encoded\n",
    "df_encoded = ohe.fit_transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Print first 5 rows of the resulting dataset - again, this will no longer be a pandas dataframe\n",
    "print(df_encoded[:5, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Print the shape of the original DataFrame\n",
    "print(df.shape)\n",
    "\n",
    "# Print the shape of the transformed array\n",
    "print(df_encoded.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DictVectorizer\n",
    "LabelEncoder followed by OneHotEncoder can be simplified by using a [DictVectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.DictVectorizer.html).\n",
    "\n",
    "Using a DictVectorizer on a DataFrame that has been converted to a dictionary yields label encoding as well as one-hot encoding in one go.\n",
    "\n",
    "When feature values are strings, this transformer will do a binary one-hot (aka one-of-K) coding: one boolean-valued feature is constructed for each of the possible string values that the feature can take on.  However, note that this transformer will only do a binary one-hot encoding when feature values are of type string. If categorical features are represented as numeric values such as int, the DictVectorizer can be followed by OneHotEncoder to complete binary one-hot encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# required to submit soln in DataCamp shell\n",
    "df = ames_housing_data.copy()\n",
    "\n",
    "# Fill missing values with 0\n",
    "df.LotFrontage = df.LotFrontage.fillna(0)\n",
    "\n",
    "# Import DictVectorizer\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "# Convert df into a dictionary: df_dict\n",
    "df_dict = df.to_dict('records')\n",
    "\n",
    "# Create the DictVectorizer object: dv\n",
    "dv = DictVectorizer(sparse=False)\n",
    "\n",
    "# Apply dv on df: df_encoded\n",
    "df_encoded = dv.fit_transform(df_dict)\n",
    "\n",
    "# Print the resulting first five rows\n",
    "print(df_encoded[:5,:])\n",
    "\n",
    "# Print the vocabulary\n",
    "print(dv.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing within Pipeline\n",
    "Having observed what steps need to be taken individually to properly process the Ames housing data, consider the much cleaner and more succinct DictVectorizer approach and put it alongside an XGBoostRegressor inside of a scikit-learn pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('ohe_onestep', DictVectorizer(dtype=<class 'numpy.float64'>, separator='=', sort=True,\n",
       "        sparse=False)), ('xgb_model', XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bytree=1, gamma=0, learning_rate=0.1, max_delta_step=0,\n",
       "       max_depth=3, min_ch...\n",
       "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
       "       silent=True, subsample=1))])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import necessary modules\n",
    "import pandas as pd\n",
    "#import xgboost as xgb\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "# Load data Ames, Iowa dataset from DataCamp's AWS url\n",
    "ames_housing_data = pd.read_csv(\"https://s3.amazonaws.com/assets.datacamp.com/production/course_3970/datasets/ames_unprocessed_data.csv\")\n",
    "\n",
    "#create X and y from data\n",
    "columns_rearranged = [\"MSSubClass\",\"LotFrontage\",\"LotArea\"] + ames_housing_data.columns.tolist()[7:-2] + [\"MSZoning\",\"PavedDrive\"] + ames_housing_data.columns.tolist()[4:7] + [\"SalePrice\"]\n",
    "\n",
    "ames_housing_data = ames_housing_data[columns_rearranged]\n",
    "\n",
    "X,y = ames_housing_data.iloc[:,:-1],ames_housing_data.iloc[:,-1]\n",
    "\n",
    "# Fill LotFrontage missing values with 0\n",
    "X.LotFrontage = X.LotFrontage.fillna(0)\n",
    "\n",
    "# Setup the pipeline steps: steps\n",
    "steps = [(\"ohe_onestep\", DictVectorizer(sparse=False)), (\"xgb_model\", xgb.XGBRegressor())]\n",
    "          \n",
    "# Create the pipeline: xgb_pipeline\n",
    "pipeline = Pipeline(steps = steps) \n",
    "          \n",
    "# Convert df into a dictionary: df_dict\n",
    "X = X.to_dict('records')\n",
    "              \n",
    "# Fit the pipeline\n",
    "pipeline.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-validating  XGBoost Model with Categorical Vars Encoded within Pipeline\n",
    "Evaluate a score by cross-validation of preprocessed model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10-fold RMSE:  29867.6037207\n"
     ]
    }
   ],
   "source": [
    "# Import necessary modules\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import xgboost as xgb\n",
    "\n",
    "# Load data Ames, Iowa dataset from DataCamp's AWS url\n",
    "ames_housing_data = pd.read_csv(\"https://s3.amazonaws.com/assets.datacamp.com/production/course_3970/datasets/ames_unprocessed_data.csv\")\n",
    "\n",
    "#create X and y from data\n",
    "columns_rearranged = [\"MSSubClass\",\"LotFrontage\",\"LotArea\"] + ames_housing_data.columns.tolist()[7:-2] + [\"MSZoning\",\"PavedDrive\"] + ames_housing_data.columns.tolist()[4:7] + [\"SalePrice\"]\n",
    "\n",
    "ames_housing_data = ames_housing_data[columns_rearranged]\n",
    "\n",
    "X,y = ames_housing_data.iloc[:,:-1],ames_housing_data.iloc[:,-1]\n",
    "\n",
    "# Fill LotFrontage missing values with 0\n",
    "X.LotFrontage = X.LotFrontage.fillna(0)\n",
    "\n",
    "# Setup the pipeline steps: steps\n",
    "steps = [(\"ohe_onestep\", DictVectorizer(sparse=False)),\n",
    "         (\"xgb_model\", xgb.XGBRegressor(max_depth=2, objective=\"reg:linear\"))]\n",
    "\n",
    "# Create the pipeline: xgb_pipeline\n",
    "xgb_pipeline = Pipeline(steps = steps)\n",
    "\n",
    "# Convert df into a dictionary: df_dict\n",
    "X = X.to_dict('records')\n",
    "\n",
    "# Cross-validate the model\n",
    "cross_val_scores = cross_val_score(xgb_pipeline, X, y, scoring = \"neg_mean_squared_error\", cv = 10, n_jobs = -1)\n",
    "\n",
    "# Print the 10-fold RMSE\n",
    "print(\"10-fold RMSE: \", np.mean(np.sqrt(np.abs(cross_val_scores))))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kidney disease case study I: Categorical Imputer (sklearn_pandas)\n",
    "Exploration of using pipelines with a dataset that requires significantly more wrangling. The chronic kidney disease dataset contains both categorical and numeric features, but contains lots of missing values. The goal here is to predict who has chronic kidney disease given various blood indicators as features.\n",
    "\n",
    "This example introduces a new library - __sklearn_pandas__. This lobrary permits one to chain many more processing steps inside of a pipeline than are currently supported in scikit-learn. Specifically, sklearn_pandas permits one to impute missing categorical values directly using the __Categorical_Imputer()__ class, and the __DataFrameMapper()__ class to apply any arbitrary sklearn-compatible transformer on DataFrame columns, where the resulting output can be either a NumPy array or DataFrame.\n",
    "\n",
    "A transformer called a __Dictifier__ encapsulates converting a DataFrame using .to_dict(\"records\") without you having to do it explicitly (and so that it works in a pipeline). \n",
    "\n",
    "The list of feature names is in kidney_feature_names, the target name is in kidney_target_name, the features in X, and the target in y."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task: apply the CategoricalImputer to impute all of the categorical columns in the dataset. You can refer to how the numeric imputation mapper was created as a template. Notice the keyword arguments input_df=True and df_out=True? This is so that you can work with DataFrames instead of arrays. By default, the transformers are passed a numpy array of the selected columns as input, and as a result, the output of the DataFrame mapper is also an array. Scikit-learn transformers have historically been designed to work with numpy arrays, not pandas DataFrames, even though their basic indexing interfaces are similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn_pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-ee3be7c80c7c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn_pandas\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDataFrameMapper\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mCategoricalImputer\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mImputer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mLabelBinarizer\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeature_extraction\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDictVectorizer\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'sklearn_pandas'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "import numpy as np\n",
    "\n",
    "from sklearn_pandas import DataFrameMapper, CategoricalImputer\n",
    "from sklearn.preprocessing import Imputer, LabelBinarizer\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformer called a __Dictifier__ that encapsulates converting a DataFrame using .to_dict(\"records\") \n",
    "# helper class to allow DictVectorizer to work properly in pipeline with dataframe\n",
    "class Dictifier(BaseEstimator,TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        if type(X) == pd.core.frame.DataFrame:\n",
    "            return X.to_dict(\"records\")\n",
    "        else:\n",
    "            return pd.DataFrame(X).to_dict(\"records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load data (may have to re-point once in github)\n",
    "\n",
    "kidney_columns = [\"age\",\"bp\",\"sg\",\"al\",\"su\",\"rbc\",\"pc\",\"pcc\",\"ba\",\"bgr\",\"bu\",\"sc\",\"sod\",\"pot\",\n",
    "                  \"hemo\",\"pcv\",\"wc\",\"rc\",\"htn\",\"dm\",\"cad\",\"appet\",\"pe\",\"ane\",\"class\"]\n",
    "\n",
    "kidney_data = pd.read_csv('C:\\\\Users\\\\seanf\\\\Documents\\\\XG_Boost_DataCamp\\\\data\\\\chronic_kidney_disease.csv',\n",
    "                         header = None,names = kidney_columns, na_values = \"?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# kidney_columns = kidney_columns[:5] + kidney_columns[9:18] + kidney_columns[5:9] + kidney_columns[18:]\n",
    "\n",
    "# kidney_data = kidney_data[kidney_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# remove label from list of feature names\n",
    "kidney_feature_names = kidney_data.columns.tolist()[:-1]\n",
    "\n",
    "# create list containing label (target) 'class'\n",
    "kidney_target_name = kidney_data.columns.tolist()[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# assign training data and label to X, y\n",
    "X,y = kidney_data[kidney_feature_names], kidney_data[kidney_target_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn_pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-f8b7b0724680>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# for submission to DataCamp Exercise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m# Import necessary modules\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn_pandas\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDataFrameMapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn_pandas\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mCategoricalImputer\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'sklearn_pandas'"
     ]
    }
   ],
   "source": [
    "# for submission to DataCamp Exercise\n",
    "# Import necessary modules\n",
    "from sklearn_pandas import DataFrameMapper\n",
    "from sklearn_pandas import CategoricalImputer\n",
    "\n",
    "# Check number of nulls in each feature column\n",
    "nulls_per_column = X.isnull().sum()\n",
    "print(nulls_per_column)\n",
    "\n",
    "# Create a boolean mask for categorical columns\n",
    "categorical_feature_mask = X.dtypes == object\n",
    "\n",
    "# Get list of categorical column names\n",
    "categorical_columns = X.columns[categorical_feature_mask].tolist()\n",
    "\n",
    "# Get list of non-categorical column names\n",
    "non_categorical_columns = X.columns[~categorical_feature_mask].tolist()\n",
    "\n",
    "# Apply numeric imputer\n",
    "numeric_imputation_mapper = DataFrameMapper(\n",
    "    [([numeric_feature], Imputer(strategy=\"median\")) for numeric_feature in non_categorical_columns],\n",
    "    input_df=True,\n",
    "    df_out=True\n",
    "    )\n",
    "\n",
    "# Apply categorical imputer\n",
    "categorical_imputation_mapper = DataFrameMapper(\n",
    "    [(category_feature, CategoricalImputer()) for category_feature in categorical_columns],\n",
    "    input_df=True,\n",
    "    df_out=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ModuleNotFoundError: No module named 'sklearn_pandas' <br>\n",
    "It seems conda does not have a sklearn_pandas package that supports Win 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kidney disease case study II: Feature Union (FeatureUnion)\n",
    "Having separately imputed numeric as well as categorical columns, task is now to use scikit-learn's __FeatureUnion__ to concatenate their results, which are contained in two separate transformer objects - __numeric_imputation_mapper__, and __categorical_imputation_mapper__, respectively.\n",
    "\n",
    "Just like with pipelines, pass it a list of (string, transformer) tuples, where the first half of each tuple is the name of the transformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# for submission to DataCamp Exercise\n",
    "# Import FeatureUnion\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "\n",
    "# Combine the numeric and categorical transformations\n",
    "numeric_categorical_union = FeatureUnion([\n",
    "                                          (\"num_mapper\", numeric_imputation_mapper),\n",
    "                                          (\"cat_mapper\", categorical_imputation_mapper)\n",
    "                                         ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kidney disease case study III: Full pipeline\n",
    "Piece together all of the transforms along with an XGBClassifier to build the full pipeline\n",
    "\n",
    "Besides the numeric_categorical_union created in the previous exercise, there are two other transforms needed: the Dictifier() transform and the DictVectorizer().\n",
    "\n",
    "Task: After creating the pipeline, cross-validate it to see how well it performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "import numpy as np\n",
    "\n",
    "from sklearn_pandas import DataFrameMapper, CategoricalImputer\n",
    "from sklearn.preprocessing import Imputer, LabelBinarizer\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# transformer called a __Dictifier__ that encapsulates converting a DataFrame using .to_dict(\"records\") \n",
    "# helper class to allow DictVectorizer to work properly in pipeline with dataframe\n",
    "class Dictifier(BaseEstimator,TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        if type(X) == pd.core.frame.DataFrame:\n",
    "            return X.to_dict(\"records\")\n",
    "        else:\n",
    "            return pd.DataFrame(X).to_dict(\"records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load data (may have to re-point once in github)\n",
    "\n",
    "kidney_columns = [\"age\",\"bp\",\"sg\",\"al\",\"su\",\"rbc\",\"pc\",\"pcc\",\"ba\",\"bgr\",\"bu\",\"sc\",\"sod\",\"pot\",\n",
    "                  \"hemo\",\"pcv\",\"wc\",\"rc\",\"htn\",\"dm\",\"cad\",\"appet\",\"pe\",\"ane\",\"class\"]\n",
    "\n",
    "kidney_data = pd.read_csv('C:\\\\Users\\\\seanf\\\\Documents\\\\XG_Boost_DataCamp\\\\data\\\\chronic_kidney_disease.csv',\n",
    "                         header = None,names = kidney_columns, na_values = \"?\")\n",
    "\n",
    "# remove label from list of feature names\n",
    "kidney_feature_names = kidney_data.columns.tolist()[:-1]\n",
    "\n",
    "# create list containing label (target) 'class'\n",
    "kidney_target_name = kidney_data.columns.tolist()[-1]\n",
    "\n",
    "# assign training data and label to X, y\n",
    "X,y = kidney_data[kidney_feature_names], kidney_data[kidney_target_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create full pipeline\n",
    "pipeline = Pipeline([\n",
    "    (\"featureunion\", numeric_categorical_union),\n",
    "    (\"dictifier\", Dictifier()), \n",
    "    ('vectorizer', DictVectorizer(sort=False)),\n",
    "    ('clf',xgb.XGBClassifier()),\n",
    "    ])\n",
    "\n",
    "# Perform cross-validation\n",
    "cross_val_scores = cross_val_score(estimator = pipeline, \n",
    "                                   X = kidney_data, \n",
    "                                   y = y, \n",
    "                                   scoring =\"roc_auc\", \n",
    "                                   cv = 3)\n",
    "\n",
    "# Print avg. AUC\n",
    "print(\"3-fold AUC: \", np.mean(cross_val_scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning XGBoost Hyperparameters in a Pipeline II\n",
    "Perform a randomized search and identify the best hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create the parameter grid\n",
    "gbm_param_grid = {\n",
    "    'clf__learning_rate': np.arange(0.05, 1, 0.05),\n",
    "    'clf__max_depth': np.arange(3, 10, 1),\n",
    "    'clf__n_estimators': np.arange(50, 200, 50)\n",
    "}\n",
    "\n",
    "# Perform RandomizedSearchCV\n",
    "randomized_roc_auc = RandomizedSearchCV(estimator = pipeline, \n",
    "                                        param_distributions = gbm_param_grid, \n",
    "                                        cv = 2, \n",
    "                                        n_iter = 2, \n",
    "                                        verbose = 1, \n",
    "                                        scoring = 'roc_auc')\n",
    "\n",
    "# Fit the estimator\n",
    "randomized_roc_auc.fit(X, y)\n",
    "\n",
    "# Compute metrics\n",
    "print(\"Best score: \", randomized_roc_auc.best_score_)\n",
    "print(\"Best model: \", randomized_roc_auc.best_estimator_)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
